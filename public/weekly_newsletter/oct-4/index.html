<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Newsletter - Oct 4 | Pk blog</title>
<meta name=keywords content="weekly-newsletter,oct-4"><meta name=description content="Weekly Newsletter - Oct 4"><meta name=author content="Prakyath Kantharaju"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/weekly_newsletter/oct-4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Weekly Newsletter - Oct 4"><meta property="og:description" content="Weekly Newsletter - Oct 4"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/weekly_newsletter/oct-4/"><meta property="og:image" content="http://localhost:1313/%3Cimage%20path/url%3E"><meta property="article:section" content="weekly_newsletter"><meta property="article:published_time" content="2020-09-15T11:30:03+00:00"><meta property="article:modified_time" content="2020-09-15T11:30:03+00:00"><meta property="og:site_name" content="PK's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:title content="Weekly Newsletter - Oct 4"><meta name=twitter:description content="Weekly Newsletter - Oct 4"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Weekly_newsletters","item":"http://localhost:1313/weekly_newsletter/"},{"@type":"ListItem","position":2,"name":"Weekly Newsletter - Oct 4","item":"http://localhost:1313/weekly_newsletter/oct-4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Newsletter - Oct 4","name":"Weekly Newsletter - Oct 4","description":"Weekly Newsletter - Oct 4","keywords":["weekly-newsletter","oct-4"],"articleBody":" Highlights OpenAI introduces Canvas: A new interface for enhanced collaboration in writing and coding projects, rolling out globally. Meta Movie Gen: A media foundation model for HD video generation with audio, showcasing state-of-the-art advancements. Google Hires OpenAI’s Sora Founder: Google DeepMind recruits lead scientist behind Sora. NVIDIA’s NVLM 1.0: New multimodal models excel at vision-language tasks, rivaling top proprietary models. New Tools: Torch’s ao for quantization and MinerU for comprehensive data extraction. Learning Resources: Cuda Mode meetup and Torch conference videos covering the latest in LLMs and multimodal systems. OpenAI News Canvas: A New Collaboration Interface OpenAI introduces Canvas, an advanced interface for working with ChatGPT on projects that require more than a simple chat. Canvas supports detailed editing, context-based suggestions, and offers tools for writing and coding enhancements. Initially available to ChatGPT Plus, Team, Enterprise, and Edu users, Canvas will eventually be accessible to all users. It helps improve project collaboration by enabling inline editing, feedback, and version control. Canvas launches automatically in applicable scenarios or via the prompt “use canvas.” Read more here Canvas includes various shortcuts:\nWriting: Adjust content length, reading level, grammar, add polish, or suggest edits. Coding: Inline code reviews, debugging support, comments, bug fixes, and language conversion. Canvas is trained to trigger contextually for different tasks, distinguishing between writing and coding needs. Model evaluations show an 83% accuracy rate for correct canvas usage, outperforming earlier baselines. As Canvas develops, the focus remains on improving collaboration and user interface efficiency.\nFunding Update OpenAI secured $6.6 billion in funding at a $157 billion valuation, boosting its frontier AI research, compute capacity, and tool development. This expansion supports OpenAI’s mission to make advanced intelligence widely accessible while working with global partners to shape AI’s positive future impact. Read more here\nMeta: Movie Gen Meta has introduced Movie Gen, a set of foundational models capable of generating 1080p HD videos with audio synchronization. The models cover a wide range of tasks, including text-to-video synthesis, video editing, and video personalization. The largest model, a 30-billion parameter transformer, supports a 73,000 video token context for 16-second videos at 16 fps. The paper outlines innovative architectural design, scaling methods, and training protocols that advance media generation capabilities. Read more here.\nGoogle Hires OpenAI’s Sora Founder Google DeepMind has recruited the lead scientist behind OpenAI’s yet-to-be-released Sora video generation model, known for his work on InstructPix2Pix. In a tweet, Demis Hassabis emphasized the goal of building a “world model.” Tweet link.\nNVIDIA’s New Vision-Language Models: NVLM 1.0 NVIDIA introduces NVLM 1.0, a family of multimodal large language models that perform at the frontier of vision-language tasks, rivaling models like GPT-4o. NVLM 1.0 uses a hybrid architecture that blends the strengths of decoder-only models and cross-attention-based approaches, resulting in state-of-the-art reasoning capabilities across multiple domains, including OCR and multimodal math. By integrating both text and multimodal datasets, NVLM enhances text-only performance while offering production-grade multimodality. Model weights and code will be released for open research. More details here.\nVLM architectures tested in the paper Please refer to the paper for more details on the performance of each architecture, paper link here. Benchmarking Fun Tools and Libraries Torch’s ao Library: Facilitates model quantization with built-in support for Torch Compile and FSDP2. GitHub link. MinerU: Comprehensive, open-source data extraction tool for PDFs, webpages, and e-books. GitHub link. Videos and Learning Resources Cuda Mode Meetup Keynote: Andrej Karpathy on LLM.c, vLLM, and more. Watch here. Torch Conference Keynote: Covers the evolution of LLM architectures. Watch here. ","wordCount":"580","inLanguage":"en","image":"http://localhost:1313/%3Cimage%20path/url%3E","datePublished":"2020-09-15T11:30:03Z","dateModified":"2020-09-15T11:30:03Z","author":{"@type":"Person","name":"Prakyath Kantharaju"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/weekly_newsletter/oct-4/"},"publisher":{"@type":"Organization","name":"Pk blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Prakyath Kantharaju's Blog (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Prakyath Kantharaju's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=https://github.com/prakyathkantharaju title=github.com><span>github.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Weekly Newsletter - Oct 4</h1><div class=post-description>Weekly Newsletter - Oct 4</div><div class=post-meta><span title='2020-09-15 11:30:03 +0000 +0000'>September 15, 2020</span>&nbsp;·&nbsp;580 words&nbsp;·&nbsp;Prakyath Kantharaju&nbsp;|&nbsp;<a href=https://github.com/Prakyathkantharaju/personal_blog/tree/main/content/weekly_newsletter/weekly_newsletter/oct-4.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#highlights aria-label=Highlights>Highlights</a></li><li><a href=#openai-news aria-label="OpenAI News">OpenAI News</a><ul><li><a href=#canvas-a-new-collaboration-interface aria-label="Canvas: A New Collaboration Interface">Canvas: A New Collaboration Interface</a></li><li><a href=#funding-update aria-label="Funding Update">Funding Update</a></li></ul></li><li><a href=#meta-movie-gen aria-label="Meta: Movie Gen">Meta: Movie Gen</a></li><li><a href=#google-hires-openais-sora-founder aria-label="Google Hires OpenAI&rsquo;s Sora Founder">Google Hires OpenAI&rsquo;s Sora Founder</a></li><li><a href=#nvidias-new-vision-language-models-nvlm-10 aria-label="NVIDIA&rsquo;s New Vision-Language Models: NVLM 1.0">NVIDIA&rsquo;s New Vision-Language Models: NVLM 1.0</a><ul><ul><li><a href=#vlm-architectures-tested-in-the-paper aria-label="VLM architectures tested in the paper">VLM architectures tested in the paper</a></li><li><a href=#benchmarking aria-label=Benchmarking>Benchmarking</a></li></ul></ul></li><li><a href=#fun-tools-and-libraries aria-label="Fun Tools and Libraries">Fun Tools and Libraries</a></li><li><a href=#videos-and-learning-resources aria-label="Videos and Learning Resources">Videos and Learning Resources</a></li></ul></div></details></div><div class=post-content><hr><h1 id=highlights>Highlights<a hidden class=anchor aria-hidden=true href=#highlights>#</a></h1><ul><li><strong>OpenAI introduces Canvas</strong>: A new interface for enhanced collaboration in writing and coding projects, rolling out globally.</li><li><strong>Meta Movie Gen</strong>: A media foundation model for HD video generation with audio, showcasing state-of-the-art advancements.</li><li><strong>Google Hires OpenAI&rsquo;s Sora Founder</strong>: Google DeepMind recruits lead scientist behind Sora.</li><li><strong>NVIDIA&rsquo;s NVLM 1.0</strong>: New multimodal models excel at vision-language tasks, rivaling top proprietary models.</li><li><strong>New Tools</strong>: Torch&rsquo;s ao for quantization and MinerU for comprehensive data extraction.</li><li><strong>Learning Resources</strong>: Cuda Mode meetup and Torch conference videos covering the latest in LLMs and multimodal systems.</li></ul><hr><h1 id=openai-news>OpenAI News<a hidden class=anchor aria-hidden=true href=#openai-news>#</a></h1><h2 id=canvas-a-new-collaboration-interface>Canvas: A New Collaboration Interface<a hidden class=anchor aria-hidden=true href=#canvas-a-new-collaboration-interface>#</a></h2><p>OpenAI introduces Canvas, an advanced interface for working with ChatGPT on projects that require more than a simple chat. Canvas supports detailed editing, context-based suggestions, and offers tools for writing and coding enhancements. Initially available to ChatGPT Plus, Team, Enterprise, and Edu users, Canvas will eventually be accessible to all users. It helps improve project collaboration by enabling inline editing, feedback, and version control. Canvas launches automatically in applicable scenarios or via the prompt &ldquo;use canvas.&rdquo;
<a href=https://openai.com/index/introducing-canvas>Read more here</a>
<img loading=lazy src=/oct-4/Canvas_Hero.webp alt=Canvas></p><p>Canvas includes various shortcuts:</p><ul><li><strong>Writing</strong>: Adjust content length, reading level, grammar, add polish, or suggest edits.</li><li><strong>Coding</strong>: Inline code reviews, debugging support, comments, bug fixes, and language conversion.</li></ul><p>Canvas is trained to trigger contextually for different tasks, distinguishing between writing and coding needs. Model evaluations show an 83% accuracy rate for correct canvas usage, outperforming earlier baselines. As Canvas develops, the focus remains on improving collaboration and user interface efficiency.</p><h2 id=funding-update>Funding Update<a hidden class=anchor aria-hidden=true href=#funding-update>#</a></h2><p>OpenAI secured $6.6 billion in funding at a $157 billion valuation, boosting its frontier AI research, compute capacity, and tool development. This expansion supports OpenAI’s mission to make advanced intelligence widely accessible while working with global partners to shape AI&rsquo;s positive future impact.
<a href=https://openai.com/index/scale-the-benefits-of-ai/>Read more here</a></p><hr><h1 id=meta-movie-gen>Meta: Movie Gen<a hidden class=anchor aria-hidden=true href=#meta-movie-gen>#</a></h1><p>Meta has introduced Movie Gen, a set of foundational models capable of generating 1080p HD videos with audio synchronization. The models cover a wide range of tasks, including text-to-video synthesis, video editing, and video personalization. The largest model, a 30-billion parameter transformer, supports a 73,000 video token context for 16-second videos at 16 fps. The paper outlines innovative architectural design, scaling methods, and training protocols that advance media generation capabilities. <a href=https://ai.meta.com/static-resource/movie-gen-research-paper>Read more here</a>.</p><p><img loading=lazy src=/oct-4/videoframe_2246.png alt="Movie Gen"></p><hr><h1 id=google-hires-openais-sora-founder>Google Hires OpenAI&rsquo;s Sora Founder<a hidden class=anchor aria-hidden=true href=#google-hires-openais-sora-founder>#</a></h1><p>Google DeepMind has recruited the lead scientist behind OpenAI&rsquo;s yet-to-be-released Sora video generation model, known for his work on InstructPix2Pix. In a tweet, Demis Hassabis emphasized the goal of building a &ldquo;world model.&rdquo; <a href=https://x.com/demishassabis/status/1841984103312208037>Tweet link</a>.</p><p><img loading=lazy src=/oct-4/Pasted%20image%2020241004093735.png alt=Sora></p><hr><h1 id=nvidias-new-vision-language-models-nvlm-10>NVIDIA&rsquo;s New Vision-Language Models: NVLM 1.0<a hidden class=anchor aria-hidden=true href=#nvidias-new-vision-language-models-nvlm-10>#</a></h1><p>NVIDIA introduces NVLM 1.0, a family of multimodal large language models that perform at the frontier of vision-language tasks, rivaling models like GPT-4o. NVLM 1.0 uses a hybrid architecture that blends the strengths of decoder-only models and cross-attention-based approaches, resulting in state-of-the-art reasoning capabilities across multiple domains, including OCR and multimodal math. By integrating both text and multimodal datasets, NVLM enhances text-only performance while offering production-grade multimodality. Model weights and code will be released for open research. <a href=https://nvlm-project.github.io/>More details here</a>.</p><h3 id=vlm-architectures-tested-in-the-paper>VLM architectures tested in the paper<a hidden class=anchor aria-hidden=true href=#vlm-architectures-tested-in-the-paper>#</a></h3><p>Please refer to the paper for more details on the performance of each architecture, <a href=https://nvlm-project.github.io/>paper link here</a>.
<img loading=lazy src=/oct-4/Pasted%20image%2020241004094511.png alt="NVLM 3"></p><h3 id=benchmarking>Benchmarking<a hidden class=anchor aria-hidden=true href=#benchmarking>#</a></h3><p><img loading=lazy src=/oct-4/Pasted%20image%2020241004094615.png alt="NVLM Benchmark"></p><hr><h1 id=fun-tools-and-libraries>Fun Tools and Libraries<a hidden class=anchor aria-hidden=true href=#fun-tools-and-libraries>#</a></h1><ul><li><strong>Torch&rsquo;s ao Library</strong>: Facilitates model quantization with built-in support for Torch Compile and FSDP2. <a href=https://github.com/pytorch/ao>GitHub link</a>.</li><li><strong>MinerU</strong>: Comprehensive, open-source data extraction tool for PDFs, webpages, and e-books. <a href=https://github.com/opendatalab/MinerU>GitHub link</a>.</li></ul><hr><h1 id=videos-and-learning-resources>Videos and Learning Resources<a hidden class=anchor aria-hidden=true href=#videos-and-learning-resources>#</a></h1><ul><li><strong>Cuda Mode Meetup Keynote</strong>: Andrej Karpathy on LLM.c, vLLM, and more. <a href="https://youtu.be/FH5wiwOyPX4?si=yj3u44fns72mpig2">Watch here</a>.</li><li><strong>Torch Conference Keynote</strong>: Covers the evolution of LLM architectures. <a href="https://youtu.be/frkAt-gZVjc?si=EwGbD3xHvNdlKGtf">Watch here</a>.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/weekly-newsletter/>Weekly-Newsletter</a></li><li><a href=http://localhost:1313/tags/oct-4/>Oct-4</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/weekly_newsletter/oct-19/><span class=title>« Prev</span><br><span>My 1st post</span>
</a><a class=next href=http://localhost:1313/weekly_newsletter/sept-20/><span class=title>Next »</span><br><span>Weekly Newsletter: Sep 20th</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Pk blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>