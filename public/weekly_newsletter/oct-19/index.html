<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Weekly Newsletter - October 19 | Pk blog</title>
<meta name=keywords content="Pytorch,LLM,Nvidia,Mistral AI"><meta name=description content="LLM Quantization, Pytorch 2.5 Release, Nvidia's Nemotron-70B Model Release, Ministral Models, and Spirit LM"><meta name=author content="Prakyath Kantharaju"><link rel=canonical href=https://prakyath.com/weekly_newsletter/oct-19><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/weekly_newsletter/oct-19/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Weekly Newsletter - October 19"><meta property="og:description" content="LLM Quantization, Pytorch 2.5 Release, Nvidia's Nemotron-70B Model Release, Ministral Models, and Spirit LM"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/weekly_newsletter/oct-19/"><meta property="og:image" content="http://localhost:1313/%3Cimage%20path/url%3E"><meta property="article:section" content="weekly_newsletter"><meta property="article:published_time" content="2024-10-19T11:30:03+00:00"><meta property="article:modified_time" content="2024-10-19T11:30:03+00:00"><meta property="og:site_name" content="PK's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:title content="Weekly Newsletter - October 19"><meta name=twitter:description content="LLM Quantization, Pytorch 2.5 Release, Nvidia's Nemotron-70B Model Release, Ministral Models, and Spirit LM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Weekly_newsletters","item":"http://localhost:1313/weekly_newsletter/"},{"@type":"ListItem","position":2,"name":"Weekly Newsletter - October 19","item":"http://localhost:1313/weekly_newsletter/oct-19/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Weekly Newsletter - October 19","name":"Weekly Newsletter - October 19","description":"LLM Quantization, Pytorch 2.5 Release, Nvidia's Nemotron-70B Model Release, Ministral Models, and Spirit LM","keywords":["Pytorch","LLM","Nvidia","Mistral AI"],"articleBody":"AI Newsletter - October 2024 1. Impact of Quantization on Large Language Models Recent extensive testing of the Llama 3.1 series models has shed light on the critical role of quantization in optimizing LLM deployments. The evaluation, conducted across various model sizes (8B, 70B, and 405B), compared three quantization schemes against the baseline 16-bit model:\nW8A8-INT: 8-bit integer quantization for weights and activations W8A8-FP: 8-bit floating-point quantization for weights and activations W4A16-INT: 4-bit integer quantization for weights, 16-bit precision for activations Key findings include:\nW8A8 schemes (both INT and FP) achieved ~2x model size compression and 1.8x performance speedup in multi-request scenarios.\nW4A16-INT provided ~3.5x model size compression and 2.4x speedup for single-stream scenarios, making it ideal for latency-critical applications.\nLarger models (70B, 405B) showed negligible performance degradation post-quantization as shown in the following figures: Smaller models (8B) experienced slight variability but maintained core semantic meaning and structural coherence as shown in the hard benchmarks above.\nThese results demonstrate that quantized models can maintain accuracy and quality compared to full-precision counterparts, offering significant computational savings and faster inference speeds. This makes quantization an essential tool for optimizing LLMs in real-world deployments, especially in resource-constrained environments or high-throughput scenarios.\nlink to the artcle\n2. PyTorch 2.5 Release Highlights PyTorch 2.5 brings several significant improvements and new features to the popular deep learning framework. Key highlights include:\nCuDNN backend for SDPA: This new backend for scaled dot product attention can provide up to 75% speed-up over FlashAttentionV2 on NVIDIA H100 GPUs, enabled by default for SDPA on H100 or newer GPUs.\nRegional compilation for torch.compile: This feature allows compiling repeated nn.Modules (e.g., transformer layers in LLMs) without recompilations, reducing compilation latencies with only a 1-5% performance trade-off compared to full model compilation.\nTorchInductor CPU backend optimization: Advancements include CPP backend code generation, FX fusions with customized CPU kernels, and support for vectorization of common data types and all Inductor IR operations. It’s compatible with both Linux and Windows, supporting Python and CPP wrappers, and AOT-Inductor mode.\nFlexAttention (Prototype): A flexible API for implementing various attention mechanisms, leveraging torch.compile to generate fused FlashAttention kernels, eliminating extra memory allocation and achieving performance comparable to handwritten implementations.\nCompiled Autograd (Prototype): An extension to the PT2 stack allowing capture of the entire backward pass, deferred until backward execution time, making it resilient to forward pass graph breaks.\nLink to the release\n3. Nvidia’s Nemotron-70B Model Release Nvidia has released Llama-3.1-Nemotron-70B-Instruct, a large language model specifically customized to enhance the helpfulness of LLM-generated responses to user queries. This model represents a significant advancement in the field of natural language processing and generation.\nKey performance metrics:\nArena Hard score: 85.0 AlpacaEval 2 LC: 57.6 GPT-4-Turbo MT-Bench: 8.98 As of October 1, 2024, Nemotron-70B claims the top position across all three automatic alignment benchmarks, surpassing formidable models like GPT-4o and Claude 3.5 Sonnet.\nTraining methodology:\nBase model: Llama-3.1-70B-Instruct Fine-tuning: RLHF (specifically REINFORCE) Reward model: Llama-3.1-Nemotron-70B-Reward Training data: HelpSteer2-Preference prompts The model demonstrates improved reasoning capabilities, evidenced by its ability to correctly answer questions like “How many r in strawberry?” without specialized prompting or additional reasoning tokens.\nNvidia has also provided a model conversion to the HuggingFace Transformers format (Llama-3.1-Nemotron-70B-Instruct-HF) and offers hosted inference with an OpenAI-compatible API interface at build.nvidia.com.\nThis release underscores Nvidia’s commitment to pushing the boundaries of LLM performance and usability, potentially reshaping the landscape of AI-powered natural language understanding and generation.\nlink to the article\n4. Ministral Models: Advancements in Edge Computing from Mistral AI Mistral AI has introduced two new state-of-the-art models for on-device computing and edge use cases: Ministral 3B and Ministral 8B, collectively known as “les Ministraux.” These models mark a significant advancement in the sub-10B category, offering impressive capabilities in knowledge, commonsense reasoning, and function-calling.\nKey features:\nSupport for up to 128k context length (currently 32k on vLLM) Ministral 8B features a special interleaved sliding-window attention pattern for faster and memory-efficient inference These models are designed to be versatile, suitable for various applications from orchestrating agentic workflows to creating specialist task workers. Their compact size and efficiency make them particularly valuable for edge computing scenarios where computational resources may be limited.\nThe release of les Ministraux on the first anniversary of Mistral 7B’s debut underscores the rapid pace of innovation in edge AI, potentially democratizing access to powerful language models for a wide range of devices and applications.\nlink to the article\n5. Spirit LM: Bridging Text and Speech in Language Models Spirit LM represents a groundbreaking approach to multimodal language modeling, seamlessly integrating text and speech capabilities. Based on a 7B pretrained text language model, Spirit LM extends its functionality to the speech domain through continuous training on both text and speech units.\nKey features:\nUnified token stream: Speech and text sequences are concatenated into a single stream of tokens. Word-level interleaving: Training utilizes a word-level interleaving method with an automatically-curated speech-text parallel corpus. Two versions: Base: Uses speech phonetic units (HuBERT) Expressive: Incorporates pitch and style units for enhanced expressivity Spirit LM demonstrates both the semantic prowess of text models and the expressive capabilities of speech models. Notably, it exhibits few-shot learning capabilities across modalities, including Automatic Speech Recognition (ASR), Text-to-Speech (TTS), and Speech Classification.\nThis innovation opens up new possibilities for more natural and expressive human-computer interactions, potentially revolutionizing applications in voice assistants, accessibility technologies, and multimedia content creation.\nlink to the article\n","wordCount":"895","inLanguage":"en","image":"http://localhost:1313/%3Cimage%20path/url%3E","datePublished":"2024-10-19T11:30:03Z","dateModified":"2024-10-19T11:30:03Z","author":{"@type":"Person","name":"Prakyath Kantharaju"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/weekly_newsletter/oct-19/"},"publisher":{"@type":"Organization","name":"Pk blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Prakyath Kantharaju's Blog (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Prakyath Kantharaju's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=https://github.com/prakyathkantharaju title=github.com><span>github.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/weekly_newsletter/>Weekly_newsletters</a></div><h1 class="post-title entry-hint-parent">Weekly Newsletter - October 19</h1><div class=post-description>LLM Quantization, Pytorch 2.5 Release, Nvidia's Nemotron-70B Model Release, Ministral Models, and Spirit LM</div><div class=post-meta><span title='2024-10-19 11:30:03 +0000 +0000'>October 19, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;895 words&nbsp;·&nbsp;Prakyath Kantharaju&nbsp;|&nbsp;<a href=https://github.com/prakyathkantharaju/personal_blog/content/weekly_newsletter/oct-19.md/weekly_newsletter/oct-19.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-impact-of-quantization-on-large-language-models>1. Impact of Quantization on Large Language Models</a></li><li><a href=#2-pytorch-25-release-highlights>2. PyTorch 2.5 Release Highlights</a></li><li><a href=#3-nvidias-nemotron-70b-model-release>3. Nvidia&rsquo;s Nemotron-70B Model Release</a></li><li><a href=#4-ministral-models-advancements-in-edge-computing-from-mistral-ai>4. Ministral Models: Advancements in Edge Computing from Mistral AI</a></li><li><a href=#5-spirit-lm-bridging-text-and-speech-in-language-models>5. Spirit LM: Bridging Text and Speech in Language Models</a></li></ul></nav></div></details></div><div class=post-content><h1 id=ai-newsletter---october-2024>AI Newsletter - October 2024<a hidden class=anchor aria-hidden=true href=#ai-newsletter---october-2024>#</a></h1><h2 id=1-impact-of-quantization-on-large-language-models>1. Impact of Quantization on Large Language Models<a hidden class=anchor aria-hidden=true href=#1-impact-of-quantization-on-large-language-models>#</a></h2><p>Recent extensive testing of the Llama 3.1 series models has shed light on the critical role of quantization in optimizing LLM deployments. The evaluation, conducted across various model sizes (8B, 70B, and 405B), compared three quantization schemes against the baseline 16-bit model:</p><ol><li>W8A8-INT: 8-bit integer quantization for weights and activations</li><li>W8A8-FP: 8-bit floating-point quantization for weights and activations</li><li>W4A16-INT: 4-bit integer quantization for weights, 16-bit precision for activations</li></ol><p>Key findings include:</p><ul><li><p>W8A8 schemes (both INT and FP) achieved ~2x model size compression and 1.8x performance speedup in multi-request scenarios.</p></li><li><p>W4A16-INT provided ~3.5x model size compression and 2.4x speedup for single-stream scenarios, making it ideal for latency-critical applications.</p></li><li><p>Larger models (70B, 405B) showed negligible performance degradation post-quantization as shown in the following figures:
<img loading=lazy src=/oct-19/CHART-LLM-Compression-Evals-ArenaHard.webp alt=images_1>
<img loading=lazy src=/oct-19/CHART-LLM-Compression-Evals-HumanEval.webp alt=images_2></p></li><li><p>Smaller models (8B) experienced slight variability but maintained core semantic meaning and structural coherence as shown in the hard benchmarks above.</p></li></ul><p>These results demonstrate that quantized models can maintain accuracy and quality compared to full-precision counterparts, offering significant computational savings and faster inference speeds. This makes quantization an essential tool for optimizing LLMs in real-world deployments, especially in resource-constrained environments or high-throughput scenarios.</p><p><a href=https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/>link to the artcle</a></p><h2 id=2-pytorch-25-release-highlights>2. PyTorch 2.5 Release Highlights<a hidden class=anchor aria-hidden=true href=#2-pytorch-25-release-highlights>#</a></h2><p>PyTorch 2.5 brings several significant improvements and new features to the popular deep learning framework. Key highlights include:</p><ol><li><p><strong>CuDNN backend for SDPA</strong>: This new backend for scaled dot product attention can provide up to 75% speed-up over FlashAttentionV2 on NVIDIA H100 GPUs, enabled by default for SDPA on H100 or newer GPUs.</p></li><li><p><strong>Regional compilation for torch.compile</strong>: This feature allows compiling repeated nn.Modules (e.g., transformer layers in LLMs) without recompilations, reducing compilation latencies with only a 1-5% performance trade-off compared to full model compilation.</p></li><li><p><strong>TorchInductor CPU backend optimization</strong>: Advancements include CPP backend code generation, FX fusions with customized CPU kernels, and support for vectorization of common data types and all Inductor IR operations. It&rsquo;s compatible with both Linux and Windows, supporting Python and CPP wrappers, and AOT-Inductor mode.</p></li><li><p><strong>FlexAttention (Prototype)</strong>: A flexible API for implementing various attention mechanisms, leveraging torch.compile to generate fused FlashAttention kernels, eliminating extra memory allocation and achieving performance comparable to handwritten implementations.</p></li><li><p><strong>Compiled Autograd (Prototype)</strong>: An extension to the PT2 stack allowing capture of the entire backward pass, deferred until backward execution time, making it resilient to forward pass graph breaks.</p></li></ol><p><a href=https://github.com/pytorch/pytorch/releases/tag/v2.5.0>Link to the release</a></p><h2 id=3-nvidias-nemotron-70b-model-release>3. Nvidia&rsquo;s Nemotron-70B Model Release<a hidden class=anchor aria-hidden=true href=#3-nvidias-nemotron-70b-model-release>#</a></h2><p>Nvidia has released Llama-3.1-Nemotron-70B-Instruct, a large language model specifically customized to enhance the helpfulness of LLM-generated responses to user queries. This model represents a significant advancement in the field of natural language processing and generation.</p><p>Key performance metrics:</p><ul><li>Arena Hard score: 85.0</li><li>AlpacaEval 2 LC: 57.6</li><li>GPT-4-Turbo MT-Bench: 8.98</li></ul><p>As of October 1, 2024, Nemotron-70B claims the top position across all three automatic alignment benchmarks, surpassing formidable models like GPT-4o and Claude 3.5 Sonnet.</p><p>Training methodology:</p><ul><li>Base model: Llama-3.1-70B-Instruct</li><li>Fine-tuning: RLHF (specifically REINFORCE)</li><li>Reward model: Llama-3.1-Nemotron-70B-Reward</li><li>Training data: HelpSteer2-Preference prompts</li></ul><p>The model demonstrates improved reasoning capabilities, evidenced by its ability to correctly answer questions like &ldquo;How many r in strawberry?&rdquo; without specialized prompting or additional reasoning tokens.</p><p>Nvidia has also provided a model conversion to the HuggingFace Transformers format (Llama-3.1-Nemotron-70B-Instruct-HF) and offers hosted inference with an OpenAI-compatible API interface at build.nvidia.com.</p><p>This release underscores Nvidia&rsquo;s commitment to pushing the boundaries of LLM performance and usability, potentially reshaping the landscape of AI-powered natural language understanding and generation.</p><p><img loading=lazy src=/oct-19/llm-nvidia-evals.png alt=images>
<a href=https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct>link to the article</a></p><h2 id=4-ministral-models-advancements-in-edge-computing-from-mistral-ai>4. Ministral Models: Advancements in Edge Computing from Mistral AI<a hidden class=anchor aria-hidden=true href=#4-ministral-models-advancements-in-edge-computing-from-mistral-ai>#</a></h2><p>Mistral AI has introduced two new state-of-the-art models for on-device computing and edge use cases: Ministral 3B and Ministral 8B, collectively known as &ldquo;les Ministraux.&rdquo; These models mark a significant advancement in the sub-10B category, offering impressive capabilities in knowledge, commonsense reasoning, and function-calling.</p><p>Key features:</p><ul><li>Support for up to 128k context length (currently 32k on vLLM)</li><li>Ministral 8B features a special interleaved sliding-window attention pattern for faster and memory-efficient inference</li></ul><p>These models are designed to be versatile, suitable for various applications from orchestrating agentic workflows to creating specialist task workers. Their compact size and efficiency make them particularly valuable for edge computing scenarios where computational resources may be limited.</p><p>The release of les Ministraux on the first anniversary of Mistral 7B&rsquo;s debut underscores the rapid pace of innovation in edge AI, potentially democratizing access to powerful language models for a wide range of devices and applications.</p><p><img loading=lazy src=/oct-19/instruct_plot_8b_with_mistral_logo.png alt=images>
<img loading=lazy src=/oct-19/instruct_table_with_gemma.png alt=images>
<a href=https://mistral.ai/news/ministraux/>link to the article</a></p><h2 id=5-spirit-lm-bridging-text-and-speech-in-language-models>5. Spirit LM: Bridging Text and Speech in Language Models<a hidden class=anchor aria-hidden=true href=#5-spirit-lm-bridging-text-and-speech-in-language-models>#</a></h2><p>Spirit LM represents a groundbreaking approach to multimodal language modeling, seamlessly integrating text and speech capabilities. Based on a 7B pretrained text language model, Spirit LM extends its functionality to the speech domain through continuous training on both text and speech units.</p><p>Key features:</p><ol><li>Unified token stream: Speech and text sequences are concatenated into a single stream of tokens.</li><li>Word-level interleaving: Training utilizes a word-level interleaving method with an automatically-curated speech-text parallel corpus.</li><li>Two versions:<ul><li>Base: Uses speech phonetic units (HuBERT)</li><li>Expressive: Incorporates pitch and style units for enhanced expressivity</li></ul></li></ol><p>Spirit LM demonstrates both the semantic prowess of text models and the expressive capabilities of speech models. Notably, it exhibits few-shot learning capabilities across modalities, including Automatic Speech Recognition (ASR), Text-to-Speech (TTS), and Speech Classification.</p><p>This innovation opens up new possibilities for more natural and expressive human-computer interactions, potentially revolutionizing applications in voice assistants, accessibility technologies, and multimedia content creation.</p><p><img loading=lazy src=/oct-19/spiritlm_overview.png alt=images>
<a href=https://speechbot.github.io/spiritlm/>link to the article</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/pytorch/>Pytorch</a></li><li><a href=http://localhost:1313/tags/llm/>LLM</a></li><li><a href=http://localhost:1313/tags/nvidia/>Nvidia</a></li><li><a href=http://localhost:1313/tags/mistral-ai/>Mistral AI</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/weekly_newsletter/oct-26/><span class=title>« Prev</span><br><span>Weekly Newsletter - October 26</span>
</a><a class=next href=http://localhost:1313/weekly_newsletter/oct-11/><span class=title>Next »</span><br><span>Weekly Newsletter - Oct 11</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Pk blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>