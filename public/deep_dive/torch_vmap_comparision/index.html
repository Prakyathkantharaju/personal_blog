<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Deep Dive: PyTorch vmap vs JAX vmap - Part 1 | Pk blog</title>
<meta name=keywords content="pytorch,vmap,JAX">
<meta name=description content="Comparing the memory and speed of PyTorch vmap and JAX vmap for dot product and attention calculation ">
<meta name=author content="Me">
<link rel=canonical href=https://canonical.url/to/page>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style>
<link rel=icon href=https://prakyathk.com/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://prakyathk.com/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://prakyathk.com/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://prakyathk.com/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://prakyathk.com/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Deep Dive: PyTorch vmap vs JAX vmap - Part 1">
<meta property="og:description" content="Comparing the memory and speed of PyTorch vmap and JAX vmap for dot product and attention calculation ">
<meta property="og:type" content="article">
<meta property="og:url" content="https://prakyathk.com/deep_dive/torch_vmap_comparision/">
<meta property="og:image" content="https://prakyathk.com/%3Cimage%20path/url%3E"><meta property="article:section" content="deep_dive">
<meta property="article:published_time" content="2020-09-15T11:30:03+00:00">
<meta property="article:modified_time" content="2020-09-15T11:30:03+00:00"><meta property="og:site_name" content="PK's Blog">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://prakyathk.com/%3Cimage%20path/url%3E">
<meta name=twitter:title content="Deep Dive: PyTorch vmap vs JAX vmap - Part 1">
<meta name=twitter:description content="Comparing the memory and speed of PyTorch vmap and JAX vmap for dot product and attention calculation ">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Deep_dives","item":"https://prakyathk.com/deep_dive/"},{"@type":"ListItem","position":2,"name":"Deep Dive: PyTorch vmap vs JAX vmap - Part 1","item":"https://prakyathk.com/deep_dive/torch_vmap_comparision/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Dive: PyTorch vmap vs JAX vmap - Part 1","name":"Deep Dive: PyTorch vmap vs JAX vmap - Part 1","description":"Comparing the memory and speed of PyTorch vmap and JAX vmap for dot product and attention calculation ","keywords":["pytorch","vmap","JAX"],"articleBody":"Deep Dive part 1: PyTorch vmap vs JAX vmap Introduction: Hello everyone, I am starting a new series of blog posts called deep dives, where I will do a deep and detailed analysis of machine learning frameworks and algorithms. Here I will break down new APIs and features of frameworks such as PyTorch, JAX, and tinygrad. In addition to that, I will also write about new algorithms in RLHF and other areas of research.\nThe first topic I will be writing about is vmap - vectorized mapping of callable functions across arrays/tensors. It returns a new function which can be called across arrays/tensors. It is a very powerful feature and most used in the XLA-based frameworks such as JAX. However, I recently discovered (slides link) that PyTorch has a similar feature called vmap through its func API layer.\nIn this blog post, I will do both memory and speed comparisons between the two framework implementations for a simple dot product operation and the attention calculation.\nThe reason why I chose to compare memory and speed for the dot product and attention operation is because I want to compare a simple operation which will require two kernel operations and a complex operation where we need multiple kernel calls such as attention. Another key reason I am writing this blog is because of the memory comparison between the two frameworks. I understand that capturing memory allocation in Python is not perfect and specifically for multi-threaded applications like ML frameworks is even harder; famous memory applications such as memory_profiler and line_profiler cannot be used due to this reason. However, I think it is still useful to get a general idea of the memory allocation of the two frameworks. To curcumvent this issue, I have captured the process ID and calculated the memory usage using psutils. If you believe that you have a better approach, please feel free to DM me or open a PR to the code repo given below.\nHere is the repo comparing the two frameworks: github link\nExperiment information Each benchmarking will be running for 1000 iterations. During each function call, I am recording the time and memory usage. As mentioned earlier, I am using psutils to capture the memory usage and will be using time.perf_counter to capture the time usage. I am also counting the GPU memory using the pynvml library, but more on the GPU performance comparison later.\nNote: I am using warm start for the jitted JAX function comparison since the first function call for JAX is always slower due to compilation. If you feel this is unfair, please feel free to remove the warmup flag in the comparison function and run the code again to compare the results.\nNote: I have also implemented the array storing version of the Jax but the results are sustainably slower compared ot the vmap and the jitted version, so I have not presented the results here. One reason for this could be the .set operation to place the value in memory, which different from pytorch’s lazy indexing. More on this in later deep dives.\nCPU For the dot product operation, I am comparing the PyTorch dot product with a for loop as suggested in the slides and the vmap version. For the JAX version, I am comparing the vmap version and the jit version of the dot product. For the attention operation, I have simplified and compared the vmap version and the jit + vmap version of the attention operation. In the future, I will also compare other implementations of JAX and PyTorch and complex operations such as model loading and switching between CPU and GPU etc.\nThe variables are declared before the dot product or attention calculation since in this blog I am concerned about the vmap comparison and not the variable declaration and initialization, etc.\nHere is the time comparison for the dot product operation on the left and the attention operation on the right. Here is the memory comparison for the dot product operation on the left and the attention operation on the right. GPU WIP……\nDiscussion and Conclusion WIP…..\n","wordCount":"683","inLanguage":"en","image":"https://prakyathk.com/%3Cimage%20path/url%3E","datePublished":"2020-09-15T11:30:03Z","dateModified":"2020-09-15T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://prakyathk.com/deep_dive/torch_vmap_comparision/"},"publisher":{"@type":"Organization","name":"Pk blog","logo":{"@type":"ImageObject","url":"https://prakyathk.com/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://prakyathk.com/ accesskey=h title="Prakyath Kantharaju's Blog (Alt + H)">
<img src=https://prakyathk.com/apple-touch-icon.png alt aria-label=logo height=35>Prakyath Kantharaju's Blog</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://prakyathk.com/about/ title=About>
<span>About</span>
</a>
</li>
<li>
<a href=https://prakyathk.com/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://github.com/prakyathkantharaju title=github.com>
<span>github.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg>
</a>
</li>
<li>
<a href=https://prakyathk.com/projects/ title=projects>
<span>projects</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://prakyathk.com/>Home</a>&nbsp;»&nbsp;<a href=https://prakyathk.com/deep_dive/>Deep_dives</a></div>
<h1 class="post-title entry-hint-parent">
Deep Dive: PyTorch vmap vs JAX vmap - Part 1
</h1>
<div class=post-description>
Comparing the memory and speed of PyTorch vmap and JAX vmap for dot product and attention calculation
</div>
<div class=post-meta><span title="2020-09-15 11:30:03 +0000 +0000">September 15, 2020</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;683 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/prakyathkantharaju/personal-blog/content/deep_dive/torch_vmap_comparision.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><nav id=TableOfContents>
<ul>
<li><a href=#introduction>Introduction:</a></li>
<li><a href=#experiment-information>Experiment information</a></li>
<li><a href=#cpu>CPU</a></li>
<li><a href=#gpu>GPU</a></li>
<li><a href=#discussion-and-conclusion>Discussion and Conclusion</a></li>
</ul>
</nav>
</div>
</details>
</div>
<div class=post-content><h1 id=deep-dive-part-1-pytorch-vmap-vs-jax-vmap>Deep Dive part 1: PyTorch vmap vs JAX vmap<a hidden class=anchor aria-hidden=true href=#deep-dive-part-1-pytorch-vmap-vs-jax-vmap>#</a></h1>
<h2 id=introduction>Introduction:<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2>
<p>Hello everyone, I am starting a new series of blog posts called <code>deep dives</code>, where I will do a deep and detailed analysis of machine learning frameworks and algorithms. Here I will break down new APIs and features of frameworks such as PyTorch, JAX, and tinygrad. In addition to that, I will also write about new algorithms in RLHF and other areas of research.</p>
<p>The first topic I will be writing about is vmap - vectorized mapping of callable functions across arrays/tensors. It returns a new function which can be called across arrays/tensors. It is a very powerful feature and most used in the XLA-based frameworks such as JAX. However, I recently discovered (slides link) that PyTorch has a similar feature called vmap through its <code>func</code> API layer.</p>
<p>In this blog post, I will do both memory and speed comparisons between the two framework implementations for a simple dot product operation and the attention calculation.</p>
<p>The reason why I chose to compare memory and speed for the dot product and attention operation is because I want to compare a simple operation which will require two kernel operations and a complex operation where we need multiple kernel calls such as attention. Another key reason I am writing this blog is because of the memory comparison between the two frameworks. I understand that capturing memory allocation in Python is not perfect and specifically for multi-threaded applications like ML frameworks is even harder; famous memory applications such as <code>memory_profiler</code> and <code>line_profiler</code> cannot be used due to this reason. However, I think it is still useful to get a general idea of the memory allocation of the two frameworks. To curcumvent this issue, I have captured the process ID and calculated the memory usage using <code>psutils</code>. If you believe that you have a better approach, please feel free to DM me or open a PR to the code repo given below.</p>
<p>Here is the repo comparing the two frameworks: <a href=https://github.com/Prakyathkantharaju/benchmark_torch_vmap_jax>github link</a></p>
<h2 id=experiment-information>Experiment information<a hidden class=anchor aria-hidden=true href=#experiment-information>#</a></h2>
<p>Each benchmarking will be running for 1000 iterations. During each function call, I am recording the time and memory usage. As mentioned earlier, I am using <code>psutils</code> to capture the memory usage and will be using <code>time.perf_counter</code> to capture the time usage. I am also counting the GPU memory using the <code>pynvml</code> library, but more on the GPU performance comparison later.</p>
<p>Note: I am using warm start for the jitted JAX function comparison since the first function call for JAX is always slower due to compilation. If you feel this is unfair, please feel free to remove the warmup flag in the comparison function and run the code again to compare the results.</p>
<p>Note: I have also implemented the array storing version of the Jax but the results are sustainably slower compared ot the vmap and the jitted version, so I have not presented the results here. One reason for this could be the <code>.set</code> operation to place the value in memory, which different from pytorch&rsquo;s lazy indexing. More on this in later deep dives.</p>
<h2 id=cpu>CPU<a hidden class=anchor aria-hidden=true href=#cpu>#</a></h2>
<p>For the dot product operation, I am comparing the PyTorch dot product with a for loop as suggested in the slides and the vmap version. For the JAX version, I am comparing the vmap version and the jit version of the dot product. For the attention operation, I have simplified and compared the vmap version and the jit + vmap version of the attention operation. In the future, I will also compare other implementations of JAX and PyTorch and complex operations such as model loading and switching between CPU and GPU etc.</p>
<p>The variables are declared before the dot product or attention calculation since in this blog I am concerned about the vmap comparison and not the variable declaration and initialization, etc.</p>
<p>Here is the time comparison for the dot product operation on the left and the attention operation on the right.
<img loading=lazy src=../images/framework_comparison.png alt="dot product time comparison">
</p>
<p>Here is the memory comparison for the dot product operation on the left and the attention operation on the right.
<img loading=lazy src=../images/framework_memory_comparison.png alt="dot product memory comparison">
</p>
<h2 id=gpu>GPU<a hidden class=anchor aria-hidden=true href=#gpu>#</a></h2>
<p>WIP&mldr;&mldr;</p>
<h2 id=discussion-and-conclusion>Discussion and Conclusion<a hidden class=anchor aria-hidden=true href=#discussion-and-conclusion>#</a></h2>
<p>WIP&mldr;..</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://prakyathk.com/tags/pytorch/>pytorch</a></li>
<li><a href=https://prakyathk.com/tags/vmap/>vmap</a></li>
<li><a href=https://prakyathk.com/tags/jax/>JAX</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=https://prakyathk.com/>Pk blog</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>